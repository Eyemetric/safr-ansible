#################
#  NVIDIA GPU Setup, install driver and container tools for GPU passthrough
#  SELinux policy
#################
- name: install the nvidia cuda repo
  register: cuda_repo
  shell: dnf config-manager --add-repo=https://developer.download.nvidia.com/compute/cuda/repos/rhel{{rhel_ver}}/x86_64/cuda-rhel{{rhel_ver}}.repo
  args:
    creates: /etc/yum.repos.d/cuda-rhel{rhel_ver}}.repo

- debug:
    var: cuda_repo

- name: enable cuda repository 
  register: cuda_enabled
  lineinfile:
    path: /etc/yum.repos.d/cuda-rhel{{rhel_ver}}.repo
    create: false
    regexp: enabled=
    line: enabled=1
  when: cuda_repo.changed == True

- name: install latest nvidia driver
  register: nvidia_install
  shell: dnf module install -y nvidia-driver:latest

- debug:
    var: nvidia_install

- name: install nvidia-container-toolkit repo 
  register: nvidia_toolkit_repo
  shell: | 
    distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
    curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.repo | tee /etc/yum.repos.d/nvidia-container-toolkit.repo
  args:
    creates: /etc/yum.repos.d/nvidia-container-toolkit.repo
- debug:
    var: nvidia_toolkit_repo

- name: install nvidia-container-toolkit package
  dnf: 
    name: nvidia-container-toolkit
    state: latest 
  when: nvidia_toolkit_repo.changed == true
###########################33
# nvidia selinux voodoo
#############################
- name: modify container-runtime-config 
  shell: sudo sed -i 's/^#no-cgroups = false/no-cgroups = true/;' /etc/nvidia-container-runtime/config.toml
  when: nvidia_toolkit_repo.changed == true
# nvidia policy shows RHEL7. we are using 8 or 9. 7 was the only policy file avail 
# so assuming that's ok. 
- name: get nvidia SELinux policy module file 
  register: nvidia_policy
  ansible.builtin.get_url:
    url: https://raw.githubusercontent.com/NVIDIA/dgx-selinux/master/bin/RHEL7/nvidia-container.pp
    dest: /tmp/nvidia-container.pp

- debug:
    var: nvidia_policy

- name: add nvidia SELinux policy module
  register: semodule_install
  command: semodule -i /tmp/nvidia-container.pp
  when: nvidia_policy.changed == True

- debug:
    var: semodule_install

- name: remove tmp nvidia policy file
  file: path=/tmp/nvidia-container.pp
        state=absent

#this is currently voodoo to me
- name: check and restore selinux labels
  register: selabels
  shell: nvidia-container-cli -k list | restorecon -v -f -

- debug:
    var: selabels

- name: restorecon all accessed devices
  register: restcon_dev
  command: restorecon -Rv /dev

- debug:
    var: restcon_dev
#end of selinux voodoo 
#should we run nvidia-smi?

###########################################################
# Podman image setup
###########################################################
- name: use podman login so we ca pull our images
  become: no
  register: dock_login
  containers.podman.podman_login:
    username: '{{ docker_user }}'
    password: '{{ docker_pwd }}'
    registry: docker.io

- name: pull gpu based paravision containers
  become: no
  register: docker_pull
  containers.podman.podman_image:
    name: '{{ item.value }}'
  loop: '{{ pv_images | dict2items }}'

- debug:
    var: docker_pull

- name: pull SAFR containers from docker.io (same cpu or gpu)
  become: no
  register: docker_pull
  containers.podman.podman_image:
    name: '{{ item.value }}'
  loop: '{{ safr_images | dict2items }}'

 # we're not currently using the var.env the config generates everything in the compose file
 # not the best way to do it bu
- name: copy config template files to app directories
  template:
    src: var.env.j2
    dest: '{{ app_dir }}/gpu/var.env'
    owner: '{{ local_user }}'
    group: '{{ local_user }}'
  tags:
    - foo
    -
- name: copy docker-compose template
  template:
    src: docker-compose.gpu.j2
    dest: '{{ app_dir }}/gpu/docker-compose.yml'
    owner: '{{ local_user }}'
    group: '{{ local_user }}'
  tags:
    - foo
